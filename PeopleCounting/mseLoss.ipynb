{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import numpy as np\n",
    "import sys\n",
    "import zipfile\n",
    "import scipy.io as sio\n",
    "import torch.utils.data as data\n",
    "import glob\n",
    "\n",
    "class UCSD(data.Dataset):\n",
    "    \"\"\"UCSD pedestrian counting data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, annotation_dir, transform=None):\n",
    "\n",
    "        self.file_list = []\n",
    "        self.file_cnts = []\n",
    "        \n",
    "        files = glob.glob(\n",
    "            os.path.join(\n",
    "                annotation_dir, \n",
    "                '*count_roi_mainwalkway.mat'))\n",
    "        \n",
    "        for f in files:\n",
    "            tmp = sio.loadmat(f)\n",
    "            \n",
    "            l_count = tmp['count'][0][0].ravel()\n",
    "            r_count = tmp['count'][0][1].ravel()\n",
    "            t_count = l_count + r_count\n",
    "            \n",
    "            [self.file_cnts.append(c) for c in t_count]\n",
    "            \n",
    "            file_parts = os.path.basename(f).split('_')\n",
    "            seq_id = \"_\".join(file_parts[0:3])\n",
    "            \n",
    "            for i in np.arange(len(t_count)):\n",
    "                \n",
    "                img_name = os.path.join(\n",
    "                        data_dir,\n",
    "                        seq_id + \".y\",\n",
    "                        \"{}_f{:03d}.png\".format(seq_id,i+1))\n",
    "                \n",
    "                img = Image.open(img_name)\n",
    "                img = img.resize((128,128))\n",
    "                \n",
    "                self.file_list.append(img)\n",
    "            \n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.file_list[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.file_cnts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "class SubsetSampler(object):\n",
    "\n",
    "    def __init__(self, subset):\n",
    "        self.subset = subset\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.subset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "class RandomSubsetSampler(object):\n",
    "\n",
    "    def __init__(self, data_source, train_share=0.8):\n",
    "\n",
    "        # Generate a list of indizes reaching from 0 ... len(data_source)-1\n",
    "        idxList = list(range(0,len(data_source)))\n",
    "\n",
    "        # Ensure that list is sorted randomly\n",
    "        random.shuffle(idxList)\n",
    "\n",
    "        # Split dataset random shares of train and test data\n",
    "        numberOfTrainSamples = int(len(data_source) / (1 / train_share))\n",
    "        \n",
    "        self.train_samples = idxList[:numberOfTrainSamples]\n",
    "        self.test_samples = idxList[numberOfTrainSamples:]\n",
    "\n",
    "\n",
    "    def trainSampler(self):\n",
    "        return SubsetSampler(self.train_samples)\n",
    "\n",
    "    def testSampler(self):\n",
    "        return SubsetSampler(self.test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Converts the 1-channel image into a 3-channel mage\n",
    "class ExpandTo3D(object):\n",
    "\n",
    "    def __call__(self, image):\n",
    "\n",
    "        width = image.size()[1]\n",
    "        height = image.size()[2]\n",
    "\n",
    "        return image.expand(3,width,height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "class CountingCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, regression_layer):\n",
    "        super(CountingCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.regression_layer = regression_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.regression_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We define different Regression Output Layer and evaluate their performance\n",
    "regression_layer1 = nn.Sequential(\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(256 * 6 * 6, 4096),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.Linear(4096, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_cnn_performance(net, max_epoch=150):\n",
    "    \n",
    "    learningRate = 0.00001\n",
    "\n",
    "    performance_statistic = []\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learningRate)\n",
    "\n",
    "    print(\"Start training\")\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for data in trainloader:\n",
    "\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            labels = labels.float()\n",
    "\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            labels = labels.view(-1,1)\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Keep track of the loss for reporting purpose\n",
    "            running_loss += loss.data[0]\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            learningRate /= 10\n",
    "            optimizer = optim.SGD(net.parameters(), lr=learningRate)\n",
    "\n",
    "        print( str(epoch) + \": \" + str(running_loss))\n",
    "\n",
    "        mae = 0.0\n",
    "        total_diff = 0.0\n",
    "        total_nr = 0\n",
    "\n",
    "        for data in testloader:\n",
    "\n",
    "            inputs, labels = data\n",
    "            labels = labels.float()\n",
    "\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = net(Variable(inputs))\n",
    "            predicted = outputs.data\n",
    "\n",
    "            # Important: Reduce dimensionality of tensor\n",
    "            # Otherwise your MAE gets screwed up\n",
    "            predicted = predicted.view(-1)\n",
    "\n",
    "            # Compute mean absolute error\n",
    "            diff = predicted - labels\n",
    "            diff = diff.abs()\n",
    "            diff = diff.sum()\n",
    "\n",
    "            total_diff += diff\n",
    "            total_nr += labels.size(0)\n",
    "\n",
    "        mae = total_diff / total_nr\n",
    "\n",
    "        print('Mean Absolute Error: %f' % mae)\n",
    "\n",
    "        # Keep track of the performance to report on performance\n",
    "        performance_statistic.append((running_loss,mae))\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    return performance_statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "transformsData = transforms.Compose([transforms.Scale(250), transforms.ToTensor()] )\n",
    "\n",
    "dataset = UCSD( data_dir='/home/ec2-user/ml_data/uni/dbs/ucsd/ucsdpeds/vidf', \n",
    "                annotation_dir='/home/ec2-user/ml_data/uni/dbs/ucsd/vidf-cvpr', \n",
    "                transform=transformsData)\n",
    "\n",
    "sampler = RandomSubsetSampler(dataset)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=sampler.trainSampler(), num_workers=2 )\n",
    "testloader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=sampler.testSampler(), num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "0: 180223.32270818783\n",
      "Mean Absolute Error: 4.823419\n",
      "1: 55480.258019303845\n",
      "Mean Absolute Error: 4.848201\n",
      "2: 48593.40744354205\n",
      "Mean Absolute Error: 4.498260\n",
      "3: 44003.711197472905\n",
      "Mean Absolute Error: 4.192979\n",
      "4: 38851.954221126085\n",
      "Mean Absolute Error: 3.778312\n",
      "5: 35653.14156527475\n",
      "Mean Absolute Error: 3.668431\n",
      "6: 32831.601036374515\n",
      "Mean Absolute Error: 3.361728\n",
      "7: 30731.249469699735\n",
      "Mean Absolute Error: 3.152181\n",
      "8: 28491.964405523995\n",
      "Mean Absolute Error: 3.130509\n",
      "9: 26599.191267182963\n",
      "Mean Absolute Error: 2.944230\n",
      "10: 25317.01302279868\n",
      "Mean Absolute Error: 2.942175\n",
      "11: 24405.49544734317\n",
      "Mean Absolute Error: 2.801876\n",
      "12: 23075.698747384566\n",
      "Mean Absolute Error: 2.671944\n",
      "13: 21389.659197947247\n",
      "Mean Absolute Error: 2.701888\n",
      "14: 21160.980486515647\n",
      "Mean Absolute Error: 2.585849\n",
      "15: 20070.69431313617\n",
      "Mean Absolute Error: 2.522069\n",
      "16: 18757.7297682598\n",
      "Mean Absolute Error: 2.428990\n",
      "17: 18488.91395787306\n",
      "Mean Absolute Error: 2.412328\n",
      "18: 18233.343036128364\n",
      "Mean Absolute Error: 2.414008\n",
      "19: 17505.2251876859\n",
      "Mean Absolute Error: 2.459916\n",
      "20: 17050.350837044116\n",
      "Mean Absolute Error: 2.361590\n",
      "21: 15975.17537614127\n",
      "Mean Absolute Error: 2.392644\n",
      "22: 15877.34796907693\n",
      "Mean Absolute Error: 2.220437\n",
      "23: 15632.883737580341\n",
      "Mean Absolute Error: 2.290198\n",
      "24: 14623.057526526594\n",
      "Mean Absolute Error: 2.171921\n",
      "25: 14417.41664698377\n",
      "Mean Absolute Error: 2.241582\n",
      "26: 14136.105191149432\n",
      "Mean Absolute Error: 2.126761\n",
      "27: 14204.133717225734\n",
      "Mean Absolute Error: 2.186885\n",
      "28: 13752.736289235734\n",
      "Mean Absolute Error: 2.152745\n",
      "29: 13938.02042371528\n",
      "Mean Absolute Error: 2.155970\n",
      "30: 12764.757839157573\n",
      "Mean Absolute Error: 2.111077\n",
      "31: 13523.43275952472\n",
      "Mean Absolute Error: 2.159057\n",
      "32: 12940.56131036082\n",
      "Mean Absolute Error: 2.066046\n",
      "33: 12660.788622836862\n",
      "Mean Absolute Error: 2.128647\n",
      "34: 12940.58986807237\n",
      "Mean Absolute Error: 2.116638\n",
      "35: 12373.61419259794\n",
      "Mean Absolute Error: 2.133451\n",
      "36: 12073.680303681762\n",
      "Mean Absolute Error: 2.096378\n",
      "37: 12309.746072466685\n",
      "Mean Absolute Error: 2.024046\n",
      "38: 11893.492169391953\n",
      "Mean Absolute Error: 2.051590\n",
      "39: 12522.39159220909\n",
      "Mean Absolute Error: 2.052308\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance of network 1\n",
    "net1 = CountingCNN(\n",
    "    regression_layer=regression_layer1\n",
    ")\n",
    "net1.cuda()\n",
    "performance_statistic = evaluate_cnn_performance(net1, max_epoch=100)\n",
    "\n",
    "# Free memory\n",
    "del net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "loss_lst, mae_lst = zip(*performance_statistic)\n",
    "\n",
    "loss_lst = list(loss_lst)\n",
    "epoch_lst = list(range(len(loss_lst)))\n",
    "mae_lst = list(mae_lst)\n",
    "\n",
    "max_acc = 100\n",
    "max_epoch = len(epoch_lst)\n",
    "max_mae = max(mae_lst)\n",
    "max_loss = 100000\n",
    "plt.figure()\n",
    "plt.plot(epoch_lst, loss_lst, 'g', linewidth=2.0)\n",
    "plt.axis([0,max_epoch,0,max_loss])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch_lst, mae_lst, 'g', linewidth=2.0)\n",
    "plt.axis([0,max_epoch,0,max_mae])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
